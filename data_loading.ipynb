{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/Desktop/programming/bp_classifier/env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "\n",
    "from comet_ml import Experiment\n",
    "\n",
    "from models.MBNV3withCBAM import MobileNetV3CBAM\n",
    "\n",
    "import config\n",
    "\n",
    "from models.functions import train, evaluate, confusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.DS_Store', 'albino', 'banana', 'butter', 'clown', 'ghi', 'hypo', 'lesser', 'mojave', 'piebald', 'spider']\n"
     ]
    }
   ],
   "source": [
    "training_data = 'images'\n",
    "classes = sorted(os.listdir(training_data))\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch. Metrics and hyperparameters can still be logged using comet_ml.log_metrics() and comet_ml.log_parameters()\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/leothesouthafrican/bp-mobilenet/ad50636519f746028c661e6a2c57f9ef\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(project_name = \"BP_MobileNet\", workspace = \"leothesouthafrican\", api_key = config.api_key)\n",
    "\n",
    "hyper_params = {\n",
    "    'learning_rate': 0.001,\n",
    "    'num_epochs': 30,\n",
    "    'batch_size': 32,\n",
    "    'image_size': 192,\n",
    "    'image_channels': 3,\n",
    "    'output_size': len(classes),\n",
    "    'num_layers': 'na',\n",
    "    'train_val_split': 0.90,\n",
    "    'device': 'mps',\n",
    "    'model_name': 'Basic MobileNetV3',\n",
    "    'criterion': 'CrossEntropyLoss',\n",
    "    'optimizer': 'Adam',\n",
    "    'dataset': 'CIFAR10',\n",
    "    'best_model_path': 'MN3Small_bp.pt',\n",
    "}\n",
    "\n",
    "#Setting the device\n",
    "device = torch.device(hyper_params['device'])\n",
    "\n",
    "# Loading model\n",
    "model = MobileNetV3CBAM(mode='large')\n",
    "model.to(device)\n",
    "\n",
    "# Setting the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device) #Setting the loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=hyper_params['learning_rate']) #Setting the optimizer\n",
    "\n",
    "# Adding model parameters to comet\n",
    "for name, param in model.named_parameters():\n",
    "    hyper_params[name] = param\n",
    "\n",
    "# Logging the hyperparameters to comet\n",
    "experiment.log_parameters(hyper_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Dataset: 5598\n",
      "Classes: ['albino', 'banana', 'butter', 'clown', 'ghi', 'hypo', 'lesser', 'mojave', 'piebald', 'spider']\n",
      "------------------------------\n",
      "Number of training images: 4478\n",
      "Number of test images: 1120\n",
      "Number of validation images: 448\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(data_path, batch_size=32, shuffle=True):\n",
    "    #load dataset\n",
    "    train_transform = transforms.Compose([\n",
    "                                        transforms.Resize((hyper_params['image_size'], hyper_params['image_size'])),\n",
    "                                        transforms.RandomHorizontalFlip(0.2),\n",
    "                                        transforms.RandomVerticalFlip(0.2),\n",
    "                                        transforms.RandomRotation(10),\n",
    "                                        transforms.ToTensor(),\n",
    "                                        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    test_transform = transforms.Compose([\n",
    "                                        transforms.Resize((hyper_params['image_size'], hyper_params['image_size'])),\n",
    "                                        transforms.ToTensor()])\n",
    "\n",
    "    full_dataset = torchvision.datasets.ImageFolder(root=data_path, transform=train_transform)\n",
    "    print('Total Images in Dataset: {}'.format(len(full_dataset)))\n",
    "    print('Classes: {}'.format(full_dataset.classes))\n",
    "    print('--' * 15)\n",
    "    #split dataset into training and validation\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_size = len(full_dataset) - train_size\n",
    "\n",
    "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
    "    print('Number of training images: {}'.format(len(train_dataset)))\n",
    "    print('Number of test images: {}'.format(len(test_dataset)))\n",
    "\n",
    "    #create dataloaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                                batch_size=hyper_params[\"batch_size\"],\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=8)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                                batch_size=hyper_params[\"batch_size\"],\n",
    "                                                shuffle=True,\n",
    "                                                num_workers=8)\n",
    "\n",
    "    n_train = int(len(train_dataset) * hyper_params['train_val_split'])\n",
    "    n_val = len(train_dataset) - n_train\n",
    "\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [n_train, n_val])\n",
    "\n",
    "    val_data = copy.deepcopy(train_dataset)\n",
    "    val_data.dataset.transform = test_transform\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(val_data,\n",
    "                                                batch_size=hyper_params[\"batch_size\"],\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=8)\n",
    "\n",
    "    print('Number of validation images: {}'.format(len(val_dataset)))\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "train_loader, val_loader, test_loader = load_dataset(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [02:09<00:00,  1.08it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.186 | Train Acc: 16.67%\n",
      "Epoch: 02 | Epoch Time: 3m 4s\n",
      "\t Val. Loss: 2.253 |  Val. Acc: 0.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.291 | Train Acc: 16.67%\n",
      "Epoch: 03 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.297 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.307 | Train Acc: 13.33%\n",
      "Epoch: 04 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.300 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.308 | Train Acc: 3.33%\n",
      "Epoch: 05 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.294 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.297 | Train Acc: 16.67%\n",
      "Epoch: 06 | Epoch Time: 2m 38s\n",
      "\t Val. Loss: 2.295 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.298 | Train Acc: 0.00%\n",
      "Epoch: 07 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.294 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.294 | Train Acc: 20.00%\n",
      "Epoch: 08 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.297 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.309 | Train Acc: 3.33%\n",
      "Epoch: 09 | Epoch Time: 2m 38s\n",
      "\t Val. Loss: 2.300 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:46<00:00,  1.31it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.295 | Train Acc: 10.00%\n",
      "Epoch: 10 | Epoch Time: 2m 41s\n",
      "\t Val. Loss: 2.298 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 140/140 [01:44<00:00,  1.34it/s]\n",
      "100%|██████████| 126/126 [00:54<00:00,  2.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss: 2.287 | Train Acc: 3.33%\n",
      "Epoch: 11 | Epoch Time: 2m 39s\n",
      "\t Val. Loss: 2.294 |  Val. Acc: 0.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/140 [00:06<14:03,  6.07s/it]"
     ]
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    train(hyper_params['num_epochs'], model, criterion, optimizer, train_loader, val_loader, hyper_params['best_model_path'], device, experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with experiment.test():\n",
    "\n",
    "    model.load_state_dict(torch.load(hyper_params['best_model_path']))\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion, device, experiment)\n",
    "\n",
    "    print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion(model, test_loader, device = device, experiment=experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "87b05828debb300a209253ec103fa105379d962da19ff6dc6c9415f115f2f596"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
